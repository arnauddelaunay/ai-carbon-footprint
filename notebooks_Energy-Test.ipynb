{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnaud/workspace/Lab/IAVerte/green-ai/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/arnaud/workspace/Lab/IAVerte/green-ai/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/arnaud/workspace/Lab/IAVerte/green-ai/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/arnaud/workspace/Lab/IAVerte/green-ai/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/arnaud/workspace/Lab/IAVerte/green-ai/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/arnaud/workspace/Lab/IAVerte/green-ai/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Training Parameters\n",
    "num_gpus = 2\n",
    "num_steps = 4000\n",
    "learning_rate = 0.001\n",
    "batch_size = 1024\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "\n",
    "# Build a convolutional neural network\n",
    "def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 5\n",
    "        x = tf.layers.conv2d(x, 64, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 256 filters and a kernel size of 5\n",
    "        x = tf.layers.conv2d(x, 256, 3, activation=tf.nn.relu)\n",
    "        # Convolution Layer with 512 filters and a kernel size of 5\n",
    "        x = tf.layers.conv2d(x, 512, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "\n",
    "        # Fully connected layer (in contrib folder for now)\n",
    "        x = tf.layers.dense(x, 2048)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=is_training)\n",
    "\n",
    "        # Fully connected layer (in contrib folder for now)\n",
    "        x = tf.layers.dense(x, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(x, n_classes)\n",
    "        # Because 'softmax_cross_entropy_with_logits' loss already apply\n",
    "        # softmax, we only apply softmax to testing network\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads\n",
    "\n",
    "\n",
    "# By default, all variables will be placed on '/gpu:0'\n",
    "# So we need a custom device function, to assign all variables to '/cpu:0'\n",
    "# Note: If GPUs are peered, '/gpu:0' can be a faster option\n",
    "PS_OPS = ['Variable', 'VariableV2', 'AutoReloadVariable']\n",
    "\n",
    "def assign_to_device(device, ps_device='/cpu:0'):\n",
    "    def _assign(op):\n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        if node_def.op in PS_OPS:\n",
    "            return \"/\" + ps_device\n",
    "        else:\n",
    "            return device\n",
    "\n",
    "    return _assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss= 2.0531, Training Accuracy= 0.339, 568 Examples/sec\n",
      "Step 10: Minibatch Loss= 0.4414, Training Accuracy= 0.899, 15771 Examples/sec\n",
      "Step 20: Minibatch Loss= 0.1711, Training Accuracy= 0.952, 19907 Examples/sec\n",
      "Step 30: Minibatch Loss= 0.1483, Training Accuracy= 0.972, 21308 Examples/sec\n",
      "Step 40: Minibatch Loss= 0.0820, Training Accuracy= 0.976, 21920 Examples/sec\n",
      "Step 50: Minibatch Loss= 0.0824, Training Accuracy= 0.978, 22267 Examples/sec\n",
      "Step 60: Minibatch Loss= 0.0523, Training Accuracy= 0.983, 22205 Examples/sec\n",
      "Step 70: Minibatch Loss= 0.0448, Training Accuracy= 0.989, 20962 Examples/sec\n",
      "Step 80: Minibatch Loss= 0.0562, Training Accuracy= 0.979, 20939 Examples/sec\n",
      "Step 90: Minibatch Loss= 0.0478, Training Accuracy= 0.991, 20979 Examples/sec\n",
      "Step 100: Minibatch Loss= 0.0365, Training Accuracy= 0.992, 21263 Examples/sec\n",
      "Step 110: Minibatch Loss= 0.0313, Training Accuracy= 0.991, 21375 Examples/sec\n",
      "Step 120: Minibatch Loss= 0.0176, Training Accuracy= 0.991, 20916 Examples/sec\n",
      "Step 130: Minibatch Loss= 0.0243, Training Accuracy= 0.993, 20776 Examples/sec\n",
      "Step 140: Minibatch Loss= 0.0110, Training Accuracy= 0.997, 20861 Examples/sec\n",
      "Step 150: Minibatch Loss= 0.0251, Training Accuracy= 0.998, 21230 Examples/sec\n",
      "Step 160: Minibatch Loss= 0.0144, Training Accuracy= 0.995, 21140 Examples/sec\n",
      "Step 170: Minibatch Loss= 0.0286, Training Accuracy= 0.994, 21162 Examples/sec\n",
      "Step 180: Minibatch Loss= 0.0284, Training Accuracy= 0.996, 21046 Examples/sec\n",
      "Step 190: Minibatch Loss= 0.0095, Training Accuracy= 0.995, 21074 Examples/sec\n",
      "Step 200: Minibatch Loss= 0.0110, Training Accuracy= 0.997, 19894 Examples/sec\n",
      "Step 210: Minibatch Loss= 0.0143, Training Accuracy= 0.998, 20990 Examples/sec\n",
      "Step 220: Minibatch Loss= 0.0175, Training Accuracy= 1.000, 20767 Examples/sec\n",
      "Step 230: Minibatch Loss= 0.0143, Training Accuracy= 0.998, 20735 Examples/sec\n",
      "Step 240: Minibatch Loss= 0.0153, Training Accuracy= 0.997, 21825 Examples/sec\n",
      "Step 250: Minibatch Loss= 0.0083, Training Accuracy= 1.000, 22156 Examples/sec\n",
      "Step 260: Minibatch Loss= 0.0133, Training Accuracy= 0.999, 20788 Examples/sec\n",
      "Step 270: Minibatch Loss= 0.0108, Training Accuracy= 1.000, 20855 Examples/sec\n",
      "Step 280: Minibatch Loss= 0.0183, Training Accuracy= 1.000, 20962 Examples/sec\n",
      "Step 290: Minibatch Loss= 0.0083, Training Accuracy= 0.998, 20833 Examples/sec\n",
      "Step 300: Minibatch Loss= 0.0111, Training Accuracy= 1.000, 21043 Examples/sec\n",
      "Step 310: Minibatch Loss= 0.0035, Training Accuracy= 0.999, 21017 Examples/sec\n",
      "Step 320: Minibatch Loss= 0.0052, Training Accuracy= 1.000, 21615 Examples/sec\n",
      "Step 330: Minibatch Loss= 0.0030, Training Accuracy= 0.999, 20835 Examples/sec\n",
      "Step 340: Minibatch Loss= 0.0043, Training Accuracy= 1.000, 20849 Examples/sec\n",
      "Step 350: Minibatch Loss= 0.0051, Training Accuracy= 0.999, 21934 Examples/sec\n",
      "Step 360: Minibatch Loss= 0.0072, Training Accuracy= 1.000, 22208 Examples/sec\n",
      "Step 370: Minibatch Loss= 0.0024, Training Accuracy= 1.000, 20894 Examples/sec\n",
      "Step 380: Minibatch Loss= 0.0049, Training Accuracy= 1.000, 21392 Examples/sec\n",
      "Step 390: Minibatch Loss= 0.0033, Training Accuracy= 1.000, 20996 Examples/sec\n",
      "Step 400: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 22088 Examples/sec\n",
      "Step 410: Minibatch Loss= 0.0076, Training Accuracy= 0.998, 20957 Examples/sec\n",
      "Step 420: Minibatch Loss= 0.0028, Training Accuracy= 0.999, 20955 Examples/sec\n",
      "Step 430: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 21041 Examples/sec\n",
      "Step 440: Minibatch Loss= 0.0061, Training Accuracy= 1.000, 21404 Examples/sec\n",
      "Step 450: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 20718 Examples/sec\n",
      "Step 460: Minibatch Loss= 0.0038, Training Accuracy= 1.000, 20807 Examples/sec\n",
      "Step 470: Minibatch Loss= 0.0051, Training Accuracy= 0.999, 20839 Examples/sec\n",
      "Step 480: Minibatch Loss= 0.0020, Training Accuracy= 1.000, 20727 Examples/sec\n",
      "Step 490: Minibatch Loss= 0.0040, Training Accuracy= 1.000, 20948 Examples/sec\n",
      "Step 500: Minibatch Loss= 0.0071, Training Accuracy= 1.000, 21231 Examples/sec\n",
      "Step 510: Minibatch Loss= 0.0010, Training Accuracy= 1.000, 20879 Examples/sec\n",
      "Step 520: Minibatch Loss= 0.0029, Training Accuracy= 1.000, 20793 Examples/sec\n",
      "Step 530: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 22065 Examples/sec\n",
      "Step 540: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 19710 Examples/sec\n",
      "Step 550: Minibatch Loss= 0.0017, Training Accuracy= 1.000, 22268 Examples/sec\n",
      "Step 560: Minibatch Loss= 0.0016, Training Accuracy= 1.000, 22184 Examples/sec\n",
      "Step 570: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 20449 Examples/sec\n",
      "Step 580: Minibatch Loss= 0.0012, Training Accuracy= 0.999, 21449 Examples/sec\n",
      "Step 590: Minibatch Loss= 0.0024, Training Accuracy= 1.000, 22152 Examples/sec\n",
      "Step 600: Minibatch Loss= 0.0031, Training Accuracy= 0.998, 21082 Examples/sec\n",
      "Step 610: Minibatch Loss= 0.0063, Training Accuracy= 1.000, 21035 Examples/sec\n",
      "Step 620: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 20861 Examples/sec\n",
      "Step 630: Minibatch Loss= 0.0029, Training Accuracy= 1.000, 20944 Examples/sec\n",
      "Step 640: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21813 Examples/sec\n",
      "Step 650: Minibatch Loss= 0.0035, Training Accuracy= 1.000, 21149 Examples/sec\n",
      "Step 660: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 21116 Examples/sec\n",
      "Step 670: Minibatch Loss= 0.0025, Training Accuracy= 1.000, 20977 Examples/sec\n",
      "Step 680: Minibatch Loss= 0.0043, Training Accuracy= 1.000, 20969 Examples/sec\n",
      "Step 690: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 20941 Examples/sec\n",
      "Step 700: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 20874 Examples/sec\n",
      "Step 710: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 21265 Examples/sec\n",
      "Step 720: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 22034 Examples/sec\n",
      "Step 730: Minibatch Loss= 0.0029, Training Accuracy= 1.000, 22278 Examples/sec\n",
      "Step 740: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 21000 Examples/sec\n",
      "Step 750: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 21191 Examples/sec\n",
      "Step 760: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 21335 Examples/sec\n",
      "Step 770: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21015 Examples/sec\n",
      "Step 780: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21210 Examples/sec\n",
      "Step 790: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 21272 Examples/sec\n",
      "Step 800: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 20952 Examples/sec\n",
      "Step 810: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21107 Examples/sec\n",
      "Step 820: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 21088 Examples/sec\n",
      "Step 830: Minibatch Loss= 0.0016, Training Accuracy= 0.999, 20950 Examples/sec\n",
      "Step 840: Minibatch Loss= 0.0044, Training Accuracy= 1.000, 20976 Examples/sec\n",
      "Step 850: Minibatch Loss= 0.0020, Training Accuracy= 1.000, 20901 Examples/sec\n",
      "Step 860: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 21040 Examples/sec\n",
      "Step 870: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21001 Examples/sec\n",
      "Step 880: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 20948 Examples/sec\n",
      "Step 890: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21273 Examples/sec\n",
      "Step 900: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21175 Examples/sec\n",
      "Step 910: Minibatch Loss= 0.0020, Training Accuracy= 0.999, 22282 Examples/sec\n",
      "Step 920: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 20900 Examples/sec\n",
      "Step 930: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 21089 Examples/sec\n",
      "Step 940: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21169 Examples/sec\n",
      "Step 950: Minibatch Loss= 0.0025, Training Accuracy= 1.000, 20864 Examples/sec\n",
      "Step 960: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21259 Examples/sec\n",
      "Step 970: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21068 Examples/sec\n",
      "Step 980: Minibatch Loss= 0.0036, Training Accuracy= 1.000, 20978 Examples/sec\n",
      "Step 990: Minibatch Loss= 0.0014, Training Accuracy= 1.000, 22040 Examples/sec\n",
      "Step 1000: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 20890 Examples/sec\n",
      "Step 1010: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21102 Examples/sec\n",
      "Step 1020: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 20971 Examples/sec\n",
      "Step 1030: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 22154 Examples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1040: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 20940 Examples/sec\n",
      "Step 1050: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21019 Examples/sec\n",
      "Step 1060: Minibatch Loss= 0.0054, Training Accuracy= 1.000, 20974 Examples/sec\n",
      "Step 1070: Minibatch Loss= 0.0028, Training Accuracy= 1.000, 22095 Examples/sec\n",
      "Step 1080: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 22091 Examples/sec\n",
      "Step 1090: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 20906 Examples/sec\n",
      "Step 1100: Minibatch Loss= 0.0088, Training Accuracy= 0.999, 20840 Examples/sec\n",
      "Step 1110: Minibatch Loss= 0.0020, Training Accuracy= 1.000, 20852 Examples/sec\n",
      "Step 1120: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 21521 Examples/sec\n",
      "Step 1130: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 20938 Examples/sec\n",
      "Step 1140: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 22070 Examples/sec\n",
      "Step 1150: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 20839 Examples/sec\n",
      "Step 1160: Minibatch Loss= 0.0030, Training Accuracy= 1.000, 21097 Examples/sec\n",
      "Step 1170: Minibatch Loss= 0.0009, Training Accuracy= 0.999, 20805 Examples/sec\n",
      "Step 1180: Minibatch Loss= 0.0049, Training Accuracy= 1.000, 20753 Examples/sec\n",
      "Step 1190: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 21201 Examples/sec\n",
      "Step 1200: Minibatch Loss= 0.0094, Training Accuracy= 1.000, 21080 Examples/sec\n",
      "Step 1210: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21003 Examples/sec\n",
      "Step 1220: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 21906 Examples/sec\n",
      "Step 1230: Minibatch Loss= 0.0022, Training Accuracy= 1.000, 20826 Examples/sec\n",
      "Step 1240: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 20934 Examples/sec\n",
      "Step 1250: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 20885 Examples/sec\n",
      "Step 1260: Minibatch Loss= 0.0017, Training Accuracy= 1.000, 21066 Examples/sec\n",
      "Step 1270: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21699 Examples/sec\n",
      "Step 1280: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 20894 Examples/sec\n",
      "Step 1290: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 20998 Examples/sec\n",
      "Step 1300: Minibatch Loss= 0.0038, Training Accuracy= 1.000, 21153 Examples/sec\n",
      "Step 1310: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 22270 Examples/sec\n",
      "Step 1320: Minibatch Loss= 0.0137, Training Accuracy= 1.000, 21121 Examples/sec\n",
      "Step 1330: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21269 Examples/sec\n",
      "Step 1340: Minibatch Loss= 0.0038, Training Accuracy= 1.000, 21127 Examples/sec\n",
      "Step 1350: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21283 Examples/sec\n",
      "Step 1360: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21056 Examples/sec\n",
      "Step 1370: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 22140 Examples/sec\n",
      "Step 1380: Minibatch Loss= 0.0026, Training Accuracy= 1.000, 21294 Examples/sec\n",
      "Step 1390: Minibatch Loss= 0.0026, Training Accuracy= 1.000, 21228 Examples/sec\n",
      "Step 1400: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21776 Examples/sec\n",
      "Step 1410: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21534 Examples/sec\n",
      "Step 1420: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 21084 Examples/sec\n",
      "Step 1430: Minibatch Loss= 0.0016, Training Accuracy= 1.000, 21101 Examples/sec\n",
      "Step 1440: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21940 Examples/sec\n",
      "Step 1450: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21121 Examples/sec\n",
      "Step 1460: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 21110 Examples/sec\n",
      "Step 1470: Minibatch Loss= 0.0025, Training Accuracy= 1.000, 21137 Examples/sec\n",
      "Step 1480: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 22161 Examples/sec\n",
      "Step 1490: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21570 Examples/sec\n",
      "Step 1500: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21221 Examples/sec\n",
      "Step 1510: Minibatch Loss= 0.0027, Training Accuracy= 1.000, 21149 Examples/sec\n",
      "Step 1520: Minibatch Loss= 0.0035, Training Accuracy= 1.000, 21137 Examples/sec\n",
      "Step 1530: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21340 Examples/sec\n",
      "Step 1540: Minibatch Loss= 0.0039, Training Accuracy= 1.000, 21915 Examples/sec\n",
      "Step 1550: Minibatch Loss= 0.0012, Training Accuracy= 1.000, 21135 Examples/sec\n",
      "Step 1560: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21371 Examples/sec\n",
      "Step 1570: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 22324 Examples/sec\n",
      "Step 1580: Minibatch Loss= 0.0027, Training Accuracy= 1.000, 21116 Examples/sec\n",
      "Step 1590: Minibatch Loss= 0.0057, Training Accuracy= 1.000, 21110 Examples/sec\n",
      "Step 1600: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 21937 Examples/sec\n",
      "Step 1610: Minibatch Loss= 0.0009, Training Accuracy= 0.999, 20888 Examples/sec\n",
      "Step 1620: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21168 Examples/sec\n",
      "Step 1630: Minibatch Loss= 0.0002, Training Accuracy= 0.999, 21287 Examples/sec\n",
      "Step 1640: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 22096 Examples/sec\n",
      "Step 1650: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21126 Examples/sec\n",
      "Step 1660: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 22496 Examples/sec\n",
      "Step 1670: Minibatch Loss= 0.0009, Training Accuracy= 1.000, 21403 Examples/sec\n",
      "Step 1680: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21141 Examples/sec\n",
      "Step 1690: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 22406 Examples/sec\n",
      "Step 1700: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 22009 Examples/sec\n",
      "Step 1710: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21180 Examples/sec\n",
      "Step 1720: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21111 Examples/sec\n",
      "Step 1730: Minibatch Loss= 0.0012, Training Accuracy= 1.000, 21276 Examples/sec\n",
      "Step 1740: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21346 Examples/sec\n",
      "Step 1750: Minibatch Loss= 0.0092, Training Accuracy= 1.000, 21195 Examples/sec\n",
      "Step 1760: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 21181 Examples/sec\n",
      "Step 1770: Minibatch Loss= 0.0020, Training Accuracy= 1.000, 21295 Examples/sec\n",
      "Step 1780: Minibatch Loss= 0.0021, Training Accuracy= 1.000, 20987 Examples/sec\n",
      "Step 1790: Minibatch Loss= 0.0017, Training Accuracy= 0.999, 21434 Examples/sec\n",
      "Step 1800: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21117 Examples/sec\n",
      "Step 1810: Minibatch Loss= 0.0038, Training Accuracy= 1.000, 21264 Examples/sec\n",
      "Step 1820: Minibatch Loss= 0.0041, Training Accuracy= 1.000, 21105 Examples/sec\n",
      "Step 1830: Minibatch Loss= 0.0033, Training Accuracy= 1.000, 21076 Examples/sec\n",
      "Step 1840: Minibatch Loss= 0.0032, Training Accuracy= 1.000, 20833 Examples/sec\n",
      "Step 1850: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 22174 Examples/sec\n",
      "Step 1860: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21056 Examples/sec\n",
      "Step 1870: Minibatch Loss= 0.0045, Training Accuracy= 1.000, 20865 Examples/sec\n",
      "Step 1880: Minibatch Loss= 0.0028, Training Accuracy= 1.000, 21102 Examples/sec\n",
      "Step 1890: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 21764 Examples/sec\n",
      "Step 1900: Minibatch Loss= 0.0014, Training Accuracy= 1.000, 20736 Examples/sec\n",
      "Step 1910: Minibatch Loss= 0.0081, Training Accuracy= 1.000, 21274 Examples/sec\n",
      "Step 1920: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21721 Examples/sec\n",
      "Step 1930: Minibatch Loss= 0.0031, Training Accuracy= 1.000, 20913 Examples/sec\n",
      "Step 1940: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21750 Examples/sec\n",
      "Step 1950: Minibatch Loss= 0.0039, Training Accuracy= 1.000, 20767 Examples/sec\n",
      "Step 1960: Minibatch Loss= 0.0020, Training Accuracy= 1.000, 20988 Examples/sec\n",
      "Step 1970: Minibatch Loss= 0.0037, Training Accuracy= 1.000, 20831 Examples/sec\n",
      "Step 1980: Minibatch Loss= 0.0014, Training Accuracy= 1.000, 21036 Examples/sec\n",
      "Step 1990: Minibatch Loss= 0.0032, Training Accuracy= 1.000, 20708 Examples/sec\n",
      "Step 2000: Minibatch Loss= 0.0017, Training Accuracy= 1.000, 20770 Examples/sec\n",
      "Step 2010: Minibatch Loss= 0.0030, Training Accuracy= 1.000, 21232 Examples/sec\n",
      "Step 2020: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 21259 Examples/sec\n",
      "Step 2030: Minibatch Loss= 0.0070, Training Accuracy= 1.000, 22322 Examples/sec\n",
      "Step 2040: Minibatch Loss= 0.0058, Training Accuracy= 1.000, 21304 Examples/sec\n",
      "Step 2050: Minibatch Loss= 0.0025, Training Accuracy= 1.000, 20983 Examples/sec\n",
      "Step 2060: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 21217 Examples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2070: Minibatch Loss= 0.0058, Training Accuracy= 1.000, 21126 Examples/sec\n",
      "Step 2080: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 20854 Examples/sec\n",
      "Step 2090: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 22003 Examples/sec\n",
      "Step 2100: Minibatch Loss= 0.0024, Training Accuracy= 1.000, 22088 Examples/sec\n",
      "Step 2110: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 21265 Examples/sec\n",
      "Step 2120: Minibatch Loss= 0.0005, Training Accuracy= 0.999, 21825 Examples/sec\n",
      "Step 2130: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 21015 Examples/sec\n",
      "Step 2140: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21219 Examples/sec\n",
      "Step 2150: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21809 Examples/sec\n",
      "Step 2160: Minibatch Loss= 0.0127, Training Accuracy= 1.000, 21041 Examples/sec\n",
      "Step 2170: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 21507 Examples/sec\n",
      "Step 2180: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21007 Examples/sec\n",
      "Step 2190: Minibatch Loss= 0.0012, Training Accuracy= 1.000, 21579 Examples/sec\n",
      "Step 2200: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21238 Examples/sec\n",
      "Step 2210: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 21306 Examples/sec\n",
      "Step 2220: Minibatch Loss= 0.0215, Training Accuracy= 1.000, 21028 Examples/sec\n",
      "Step 2230: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 22094 Examples/sec\n",
      "Step 2240: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21108 Examples/sec\n",
      "Step 2250: Minibatch Loss= 0.0041, Training Accuracy= 1.000, 21098 Examples/sec\n",
      "Step 2260: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21215 Examples/sec\n",
      "Step 2270: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 20881 Examples/sec\n",
      "Step 2280: Minibatch Loss= 0.0010, Training Accuracy= 1.000, 21014 Examples/sec\n",
      "Step 2290: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 21008 Examples/sec\n",
      "Step 2300: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21044 Examples/sec\n",
      "Step 2310: Minibatch Loss= 0.0003, Training Accuracy= 0.999, 21344 Examples/sec\n",
      "Step 2320: Minibatch Loss= 0.0064, Training Accuracy= 1.000, 20815 Examples/sec\n",
      "Step 2330: Minibatch Loss= 0.0052, Training Accuracy= 0.998, 20992 Examples/sec\n",
      "Step 2340: Minibatch Loss= 0.0015, Training Accuracy= 0.999, 21109 Examples/sec\n",
      "Step 2350: Minibatch Loss= 0.0010, Training Accuracy= 1.000, 21003 Examples/sec\n",
      "Step 2360: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21377 Examples/sec\n",
      "Step 2370: Minibatch Loss= 0.0006, Training Accuracy= 0.999, 20930 Examples/sec\n",
      "Step 2380: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 20900 Examples/sec\n",
      "Step 2390: Minibatch Loss= 0.0029, Training Accuracy= 0.999, 21350 Examples/sec\n",
      "Step 2400: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 22169 Examples/sec\n",
      "Step 2410: Minibatch Loss= 0.0064, Training Accuracy= 1.000, 20883 Examples/sec\n",
      "Step 2420: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 20992 Examples/sec\n",
      "Step 2430: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 20945 Examples/sec\n",
      "Step 2440: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 20813 Examples/sec\n",
      "Step 2450: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21364 Examples/sec\n",
      "Step 2460: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21021 Examples/sec\n",
      "Step 2470: Minibatch Loss= 0.0032, Training Accuracy= 1.000, 21236 Examples/sec\n",
      "Step 2480: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 20983 Examples/sec\n",
      "Step 2490: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21967 Examples/sec\n",
      "Step 2500: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 20876 Examples/sec\n",
      "Step 2510: Minibatch Loss= 0.0114, Training Accuracy= 1.000, 21398 Examples/sec\n",
      "Step 2520: Minibatch Loss= 0.0010, Training Accuracy= 0.999, 22429 Examples/sec\n",
      "Step 2530: Minibatch Loss= 0.0085, Training Accuracy= 1.000, 22259 Examples/sec\n",
      "Step 2540: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21304 Examples/sec\n",
      "Step 2550: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21588 Examples/sec\n",
      "Step 2560: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 20888 Examples/sec\n",
      "Step 2570: Minibatch Loss= 0.0061, Training Accuracy= 1.000, 21067 Examples/sec\n",
      "Step 2580: Minibatch Loss= 0.0016, Training Accuracy= 1.000, 20957 Examples/sec\n",
      "Step 2590: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21210 Examples/sec\n",
      "Step 2600: Minibatch Loss= 0.0053, Training Accuracy= 1.000, 20931 Examples/sec\n",
      "Step 2610: Minibatch Loss= 0.0039, Training Accuracy= 1.000, 21061 Examples/sec\n",
      "Step 2620: Minibatch Loss= 0.0004, Training Accuracy= 0.999, 20923 Examples/sec\n",
      "Step 2630: Minibatch Loss= 0.0002, Training Accuracy= 0.999, 20990 Examples/sec\n",
      "Step 2640: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21190 Examples/sec\n",
      "Step 2650: Minibatch Loss= 0.0004, Training Accuracy= 1.000, 21286 Examples/sec\n",
      "Step 2660: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 21076 Examples/sec\n",
      "Step 2670: Minibatch Loss= 0.0087, Training Accuracy= 1.000, 20801 Examples/sec\n",
      "Step 2680: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 20858 Examples/sec\n",
      "Step 2690: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 22389 Examples/sec\n",
      "Step 2700: Minibatch Loss= 0.0004, Training Accuracy= 0.999, 20984 Examples/sec\n",
      "Step 2710: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21055 Examples/sec\n",
      "Step 2720: Minibatch Loss= 0.0084, Training Accuracy= 1.000, 22194 Examples/sec\n",
      "Step 2730: Minibatch Loss= 0.0023, Training Accuracy= 1.000, 21715 Examples/sec\n",
      "Step 2740: Minibatch Loss= 0.0048, Training Accuracy= 1.000, 21335 Examples/sec\n",
      "Step 2750: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21154 Examples/sec\n",
      "Step 2760: Minibatch Loss= 0.0005, Training Accuracy= 1.000, 20936 Examples/sec\n",
      "Step 2770: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 22248 Examples/sec\n",
      "Step 2780: Minibatch Loss= 0.0020, Training Accuracy= 1.000, 20973 Examples/sec\n",
      "Step 2790: Minibatch Loss= 0.0114, Training Accuracy= 1.000, 21185 Examples/sec\n",
      "Step 2800: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21103 Examples/sec\n",
      "Step 2810: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21242 Examples/sec\n",
      "Step 2820: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 22229 Examples/sec\n",
      "Step 2830: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 21720 Examples/sec\n",
      "Step 2840: Minibatch Loss= 0.0058, Training Accuracy= 1.000, 21171 Examples/sec\n",
      "Step 2850: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 20979 Examples/sec\n",
      "Step 2860: Minibatch Loss= 0.0031, Training Accuracy= 1.000, 20950 Examples/sec\n",
      "Step 2870: Minibatch Loss= 0.0029, Training Accuracy= 1.000, 20879 Examples/sec\n",
      "Step 2880: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21936 Examples/sec\n",
      "Step 2890: Minibatch Loss= 0.0091, Training Accuracy= 1.000, 22403 Examples/sec\n",
      "Step 2900: Minibatch Loss= 0.0170, Training Accuracy= 1.000, 21138 Examples/sec\n",
      "Step 2910: Minibatch Loss= 0.0011, Training Accuracy= 0.999, 21077 Examples/sec\n",
      "Step 2920: Minibatch Loss= 0.0056, Training Accuracy= 0.999, 21112 Examples/sec\n",
      "Step 2930: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21208 Examples/sec\n",
      "Step 2940: Minibatch Loss= 0.0084, Training Accuracy= 1.000, 21023 Examples/sec\n",
      "Step 2950: Minibatch Loss= 0.0025, Training Accuracy= 1.000, 21322 Examples/sec\n",
      "Step 2960: Minibatch Loss= 0.0055, Training Accuracy= 1.000, 22396 Examples/sec\n",
      "Step 2970: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21722 Examples/sec\n",
      "Step 2980: Minibatch Loss= 0.0043, Training Accuracy= 0.999, 22304 Examples/sec\n",
      "Step 2990: Minibatch Loss= 0.0029, Training Accuracy= 1.000, 21138 Examples/sec\n",
      "Step 3000: Minibatch Loss= 0.0037, Training Accuracy= 1.000, 21844 Examples/sec\n",
      "Step 3010: Minibatch Loss= 0.0010, Training Accuracy= 1.000, 21075 Examples/sec\n",
      "Step 3020: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21091 Examples/sec\n",
      "Step 3030: Minibatch Loss= 0.0056, Training Accuracy= 1.000, 21136 Examples/sec\n",
      "Step 3040: Minibatch Loss= 0.0040, Training Accuracy= 1.000, 20952 Examples/sec\n",
      "Step 3050: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21104 Examples/sec\n",
      "Step 3060: Minibatch Loss= 0.0136, Training Accuracy= 1.000, 21918 Examples/sec\n",
      "Step 3070: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21904 Examples/sec\n",
      "Step 3080: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21459 Examples/sec\n",
      "Step 3090: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21141 Examples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3100: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 21118 Examples/sec\n",
      "Step 3110: Minibatch Loss= 0.0035, Training Accuracy= 1.000, 21338 Examples/sec\n",
      "Step 3120: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21900 Examples/sec\n",
      "Step 3130: Minibatch Loss= 0.0113, Training Accuracy= 1.000, 22278 Examples/sec\n",
      "Step 3140: Minibatch Loss= 0.0110, Training Accuracy= 1.000, 21135 Examples/sec\n",
      "Step 3150: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21133 Examples/sec\n",
      "Step 3160: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21250 Examples/sec\n",
      "Step 3170: Minibatch Loss= 0.0010, Training Accuracy= 1.000, 21151 Examples/sec\n",
      "Step 3180: Minibatch Loss= 0.0021, Training Accuracy= 0.999, 22159 Examples/sec\n",
      "Step 3190: Minibatch Loss= 0.0017, Training Accuracy= 1.000, 21102 Examples/sec\n",
      "Step 3200: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21939 Examples/sec\n",
      "Step 3210: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 20881 Examples/sec\n",
      "Step 3220: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 22475 Examples/sec\n",
      "Step 3230: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21153 Examples/sec\n",
      "Step 3240: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21114 Examples/sec\n",
      "Step 3250: Minibatch Loss= 0.0030, Training Accuracy= 1.000, 22423 Examples/sec\n",
      "Step 3260: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 22154 Examples/sec\n",
      "Step 3270: Minibatch Loss= 0.0008, Training Accuracy= 1.000, 21046 Examples/sec\n",
      "Step 3280: Minibatch Loss= 0.0011, Training Accuracy= 1.000, 21238 Examples/sec\n",
      "Step 3290: Minibatch Loss= 0.0247, Training Accuracy= 1.000, 21102 Examples/sec\n",
      "Step 3300: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 20941 Examples/sec\n",
      "Step 3310: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 22278 Examples/sec\n",
      "Step 3320: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21170 Examples/sec\n",
      "Step 3330: Minibatch Loss= 0.0028, Training Accuracy= 1.000, 21027 Examples/sec\n",
      "Step 3340: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 20970 Examples/sec\n",
      "Step 3350: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21108 Examples/sec\n",
      "Step 3360: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21170 Examples/sec\n",
      "Step 3370: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21008 Examples/sec\n",
      "Step 3380: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21129 Examples/sec\n",
      "Step 3390: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21127 Examples/sec\n",
      "Step 3400: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21055 Examples/sec\n",
      "Step 3410: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 22502 Examples/sec\n",
      "Step 3420: Minibatch Loss= 0.0085, Training Accuracy= 1.000, 21132 Examples/sec\n",
      "Step 3430: Minibatch Loss= 0.0148, Training Accuracy= 1.000, 21116 Examples/sec\n",
      "Step 3440: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21151 Examples/sec\n",
      "Step 3450: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21118 Examples/sec\n",
      "Step 3460: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21001 Examples/sec\n",
      "Step 3470: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21303 Examples/sec\n",
      "Step 3480: Minibatch Loss= 0.0158, Training Accuracy= 1.000, 21182 Examples/sec\n",
      "Step 3490: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21209 Examples/sec\n",
      "Step 3500: Minibatch Loss= 0.0112, Training Accuracy= 1.000, 21458 Examples/sec\n",
      "Step 3510: Minibatch Loss= 0.0029, Training Accuracy= 1.000, 21282 Examples/sec\n",
      "Step 3520: Minibatch Loss= 0.0017, Training Accuracy= 1.000, 21040 Examples/sec\n",
      "Step 3530: Minibatch Loss= 0.0014, Training Accuracy= 1.000, 22297 Examples/sec\n",
      "Step 3540: Minibatch Loss= 0.0086, Training Accuracy= 1.000, 22219 Examples/sec\n",
      "Step 3550: Minibatch Loss= 0.0069, Training Accuracy= 1.000, 21091 Examples/sec\n",
      "Step 3560: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 21983 Examples/sec\n",
      "Step 3570: Minibatch Loss= 0.0027, Training Accuracy= 1.000, 21357 Examples/sec\n",
      "Step 3580: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21378 Examples/sec\n",
      "Step 3590: Minibatch Loss= 0.0003, Training Accuracy= 1.000, 21124 Examples/sec\n",
      "Step 3600: Minibatch Loss= 0.0046, Training Accuracy= 1.000, 21126 Examples/sec\n",
      "Step 3610: Minibatch Loss= 0.0007, Training Accuracy= 1.000, 21432 Examples/sec\n",
      "Step 3620: Minibatch Loss= 0.0073, Training Accuracy= 1.000, 21320 Examples/sec\n",
      "Step 3630: Minibatch Loss= 0.0118, Training Accuracy= 1.000, 21034 Examples/sec\n",
      "Step 3640: Minibatch Loss= 0.0037, Training Accuracy= 1.000, 21161 Examples/sec\n",
      "Step 3650: Minibatch Loss= 0.0033, Training Accuracy= 1.000, 22334 Examples/sec\n",
      "Step 3660: Minibatch Loss= 0.0115, Training Accuracy= 0.999, 21091 Examples/sec\n",
      "Step 3670: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21092 Examples/sec\n",
      "Step 3680: Minibatch Loss= 0.0002, Training Accuracy= 1.000, 21876 Examples/sec\n",
      "Step 3690: Minibatch Loss= 0.0054, Training Accuracy= 1.000, 21083 Examples/sec\n",
      "Step 3700: Minibatch Loss= 0.0103, Training Accuracy= 1.000, 22435 Examples/sec\n",
      "Step 3710: Minibatch Loss= 0.0015, Training Accuracy= 1.000, 21049 Examples/sec\n",
      "Step 3720: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 21198 Examples/sec\n",
      "Step 3730: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21417 Examples/sec\n",
      "Step 3740: Minibatch Loss= 0.0097, Training Accuracy= 1.000, 21468 Examples/sec\n",
      "Step 3750: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21200 Examples/sec\n",
      "Step 3760: Minibatch Loss= 0.0012, Training Accuracy= 0.999, 22127 Examples/sec\n",
      "Step 3770: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21187 Examples/sec\n",
      "Step 3780: Minibatch Loss= 0.0019, Training Accuracy= 1.000, 21113 Examples/sec\n",
      "Step 3790: Minibatch Loss= 0.0001, Training Accuracy= 1.000, 21054 Examples/sec\n",
      "Step 3800: Minibatch Loss= 0.0018, Training Accuracy= 1.000, 21118 Examples/sec\n",
      "Step 3810: Minibatch Loss= 0.0002, Training Accuracy= 0.999, 21117 Examples/sec\n",
      "Step 3820: Minibatch Loss= 0.0094, Training Accuracy= 1.000, 21140 Examples/sec\n",
      "Step 3830: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21121 Examples/sec\n",
      "Step 3840: Minibatch Loss= 0.0047, Training Accuracy= 1.000, 21135 Examples/sec\n",
      "Step 3850: Minibatch Loss= 0.0030, Training Accuracy= 1.000, 22264 Examples/sec\n",
      "Step 3860: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21062 Examples/sec\n",
      "Step 3870: Minibatch Loss= 0.0032, Training Accuracy= 1.000, 21147 Examples/sec\n",
      "Step 3880: Minibatch Loss= 0.0044, Training Accuracy= 1.000, 21423 Examples/sec\n",
      "Step 3890: Minibatch Loss= 0.0138, Training Accuracy= 1.000, 21202 Examples/sec\n",
      "Step 3900: Minibatch Loss= 0.0000, Training Accuracy= 1.000, 21142 Examples/sec\n",
      "Step 3910: Minibatch Loss= 0.0067, Training Accuracy= 1.000, 22501 Examples/sec\n",
      "Step 3920: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21109 Examples/sec\n",
      "Step 3930: Minibatch Loss= 0.0013, Training Accuracy= 1.000, 22132 Examples/sec\n",
      "Step 3940: Minibatch Loss= 0.0429, Training Accuracy= 1.000, 21383 Examples/sec\n",
      "Step 3950: Minibatch Loss= 0.0006, Training Accuracy= 1.000, 21106 Examples/sec\n",
      "Step 3960: Minibatch Loss= 0.0084, Training Accuracy= 1.000, 21114 Examples/sec\n",
      "Step 3970: Minibatch Loss= 0.0035, Training Accuracy= 1.000, 21156 Examples/sec\n",
      "Step 3980: Minibatch Loss= 0.0040, Training Accuracy= 1.000, 21315 Examples/sec\n",
      "Step 3990: Minibatch Loss= 0.0068, Training Accuracy= 1.000, 21288 Examples/sec\n",
      "Step 4000: Minibatch Loss= 0.0151, Training Accuracy= 1.000, 21515 Examples/sec\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.99239874\n",
      "Total time of training : 410.5855276584625\n"
     ]
    }
   ],
   "source": [
    "# Place all ops on CPU by default\n",
    "start_time = time.time()\n",
    "with tf.device('/cpu:0'):\n",
    "    tower_grads = []\n",
    "    reuse_vars = False\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, num_input])\n",
    "    Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "\n",
    "    # Loop over all GPUs and construct their own computation graph\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(assign_to_device('/gpu:{}'.format(i), ps_device='/cpu:0')):\n",
    "\n",
    "            # Split data between GPUs\n",
    "            _x = X[i * batch_size: (i+1) * batch_size]\n",
    "            _y = Y[i * batch_size: (i+1) * batch_size]\n",
    "\n",
    "            # Because Dropout have different behavior at training and prediction time, we\n",
    "            # need to create 2 distinct computation graphs that share the same weights.\n",
    "\n",
    "            # Create a graph for training\n",
    "            logits_train = conv_net(_x, num_classes, dropout,\n",
    "                                    reuse=reuse_vars, is_training=True)\n",
    "            # Create another graph for testing that reuse the same weights\n",
    "            logits_test = conv_net(_x, num_classes, dropout,\n",
    "                                   reuse=True, is_training=False)\n",
    "\n",
    "            # Define loss and optimizer (with train logits, for dropout to take effect)\n",
    "            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits_train, labels=_y))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            grads = optimizer.compute_gradients(loss_op)\n",
    "\n",
    "            # Only first GPU compute accuracy\n",
    "            if i == 0:\n",
    "                # Evaluate model (with test logits, for dropout to be disabled)\n",
    "                correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(_y, 1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "            reuse_vars = True\n",
    "            tower_grads.append(grads)\n",
    "\n",
    "    tower_grads = average_gradients(tower_grads)\n",
    "    train_op = optimizer.apply_gradients(tower_grads)\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start Training\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        # Keep training until reach max iterations\n",
    "        for step in range(1, num_steps + 1):\n",
    "            # Get a batch for each GPU\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size * num_gpus)\n",
    "            # Run optimization op (backprop)\n",
    "            ts = time.time()\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            te = time.time() - ts\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y})\n",
    "                print(\"Step \" + str(step) + \": Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc) + \", %i Examples/sec\" % int(len(batch_x)/te))\n",
    "            step += 1\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for MNIST test images\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "            np.mean([sess.run(accuracy, feed_dict={X: mnist.test.images[i:i+batch_size],\n",
    "            Y: mnist.test.labels[i:i+batch_size]}) for i in range(0, len(mnist.test.images), batch_size)]))\n",
    "print(\"Total time of training : %s\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 446065761015725079\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 162004992\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 7238694998269149715\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 161611776\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 7633307642351845955\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "green-ai",
   "language": "python",
   "name": "green-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
